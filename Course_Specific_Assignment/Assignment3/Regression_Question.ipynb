{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Regression"
      ],
      "metadata": {
        "id": "x__lziurWGLc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "11. What is regression analysis and what is its purpose?\n"
      ],
      "metadata": {
        "id": "ATq7cZFyWR0r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Regression is a statistical method used in finance, investing, and other disciplines that attempts to determine the strength and character of the relationship between one dependent variable (usually denoted by Y) and a series of other variables (known as independent variables)."
      ],
      "metadata": {
        "id": "tx6J5UHqWird"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "12. What is the difference between simple linear regression and multiple linear regression?"
      ],
      "metadata": {
        "id": "GHatnrs-W3t6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Simple linear regression and multiple linear regression are both statistical techniques used to model the relationship between a dependent variable and one or more independent variables.\n",
        "Simple linear regression involves only one independent variable and one dependent variable, where the relationship between them is assumed to be linear.\n",
        "Multiple linear regression, on the other hand, involves more than one independent variable and one dependent variable."
      ],
      "metadata": {
        "id": "OeERGiAUW3O0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "13. How do you interpret the R-squared value in regression?"
      ],
      "metadata": {
        "id": "VUKBkScnXJt0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The R-squared value in regression represents the proportion of the variance in the dependent variable that can be explained by the independent variables. It ranges from 0 to 1, where 0 indicates that the independent variables have no explanatory power, and 1 indicates a perfect fit where all the variability in the dependent variable is explained."
      ],
      "metadata": {
        "id": "q4RyhrkTXJqq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "14. What is the difference between correlation and regression?\n",
        "\n"
      ],
      "metadata": {
        "id": "3L4oln5SXJl4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Correlation measures the strength and direction of the linear relationship between two variables and regression analyzes the relationship between a dependent variable and one or more independent variables to predict or explain the dependent variable.\n",
        "Correlation finds on the association between variables while regression understand the nature of the relationship and make predictions or inferences based on that relationship."
      ],
      "metadata": {
        "id": "sQqiQB9HXJjK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "15. What is the difference between the coefficients and the intercept in regression?\n"
      ],
      "metadata": {
        "id": "FemIumeiXJgd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Coefficients in regression also known as regression coefficients or beta coefficients. They quantify the change in the dependent variable associated with a one-unit change in the corresponding independent variable, holding all other variables constant.\n",
        "\n",
        "The intercept is also known as y-intercept. It is the predicted value of the dependent variable when all independent variables have no effect."
      ],
      "metadata": {
        "id": "JcR2MtRFXJdq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "16. How do you handle outliers in regression analysis?"
      ],
      "metadata": {
        "id": "RO7btVjCXJZn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are various techniques to handle regression analysis. The following are ways:\n",
        "  1. Trimming/Remove the outliers. In this technique, we remove the outliers from the dataset.\n",
        "  2. Quantile based flooring and capping. In this technique, the outlier is capped at a certain value above the 90th percentile value or floored at a factor below the 10th percentile value.\n",
        "  3. Mean/Median imputation.As the mean value is highly influenced by the outliers, it is advised to replace the outliers with the median value.\n",
        "  4. Transforming the data using mathematical functions can sometimes reduce the impact of outliers. Common transformations include taking the logarithm, square root, or reciprocal of the data. These transformations can help make the data more normally distributed and stabilize the variance."
      ],
      "metadata": {
        "id": "sDssvP_xXJXL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "17. What is the difference between ridge regression and ordinary least squares regression?"
      ],
      "metadata": {
        "id": "CSnj-RiuXJU6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ordinary Least Square regression estimates the regression coefficients by minimizing the sum of squared residuals. However, when multicollinearity exists, model estimates can be unstable. And highly sensitive to small changes in the data, leading to unreliable coefficient estimates.\n",
        "Ridge regression is a regularization technique that adds a penalty term to the sum of squared residuals. This penalty term, controlled by a tuning parameter called lambda (Î»), shrinks the estimated coefficients towards zero, reducing their variance. By doing so, ridge regression can be used for multicollinearity issues and provide more stable coefficient estimates."
      ],
      "metadata": {
        "id": "V_S96oz4XJSa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "18. What is heteroscedasticity in regression and how does it affect the model?"
      ],
      "metadata": {
        "id": "fLL_qQ4icY2J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Heteroscedasticity in regression refers to the unequal variability of the residuals across the range of the independent variables. It can affect the model by biasing the coefficient estimates and leading to inefficient or inconsistent standard errors. It can result in unreliable hypothesis tests and confidence intervals."
      ],
      "metadata": {
        "id": "TsKHy06fcY0C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "19. How do you handle multicollinearity in regression analysis?\n"
      ],
      "metadata": {
        "id": "u5c-ocqucYyZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are few approaches to handle multicollinearity:\n",
        "  1. Feature Selection. Choosing highly correlated independent variables.\n",
        "  2. Data Transformation. For Example PCA.\n",
        "  3. Ridge Regression. It can handle multicollinearity by adding a penalty term (lambda) to the regression objective function, reducing the impact of multicollinearity on coefficients."
      ],
      "metadata": {
        "id": "GKccLUd_cYv_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "20. What is polynomial regression and when is it used?"
      ],
      "metadata": {
        "id": "WMlAndSacYtb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Polynomial regression is a form of regression analysis that models the relationship between the independent variables and the dependent variable as an nth-degree polynomial. It is used when the relationship between the variables is nonlinear. It can capture more complex relationships and provide a better fit to the data."
      ],
      "metadata": {
        "id": "46GZjc_DXJP2"
      }
    }
  ]
}