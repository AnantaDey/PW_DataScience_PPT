{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "10. What is the K-Nearest Neighbors (KNN) algorithm?\n"
      ],
      "metadata": {
        "id": "wre0tTFSjaM3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The K-Nearest Neighbors (KNN) algorithm is a supervised machine learning algorithm used for both classification and regression tasks. It is a non-parametric algorithm, meaning it makes no assumptions about the underlying data distribution."
      ],
      "metadata": {
        "id": "QPefNpIEjaRp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "11. How does the KNN algorithm work?"
      ],
      "metadata": {
        "id": "hDb-c0dojaXF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The KNN algorithm works by classifying or predicting a new instance based on its similarity to the K nearest instances in the training data. For classification, the algorithm assigns the class label that is most frequent among the K nearest neighbors. For regression, it predicts the average or weighted average of the target values of the K nearest neighbors."
      ],
      "metadata": {
        "id": "H-A-EfpWjacI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "12. How do you choose the value of K in KNN?"
      ],
      "metadata": {
        "id": "uZXDcbkZjaiB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Choosing the value of K in KNN is important for balancing the bias-variance trade-off. A smaller value of K (e.g., K = 1) makes the model more sensitive to noise in the data and can lead to overfitting. On the other hand, a larger value of K reduces the impact of individual noisy or mislabeled instances but can lead to underfitting. The choice of K should be determined based on the characteristics of the data and the specific problem, often through cross-validation or other evaluation techniques."
      ],
      "metadata": {
        "id": "8zmrxLtIjam0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "13. What are the advantages and disadvantages of the KNN algorithm?\n"
      ],
      "metadata": {
        "id": "Yd9N4jKWjasK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Advantages of the KNN algorithm include simplicity, as it is easy to understand and implement, and its ability to handle multi-class classification and regression tasks. It is also non-parametric and can work well with nonlinear relationships in the data. However, the KNN algorithm can be computationally expensive, especially with large datasets, as it requires calculating distances for each instance. It is also sensitive to the choice of distance metric and the scaling of features."
      ],
      "metadata": {
        "id": "7FDalRXDjaxM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "14. How does the choice of distance metric affect the performance of KNN?"
      ],
      "metadata": {
        "id": "w3nykps9ja2u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The choice of distance metric affects the performance of KNN as it determines how instances are measured for similarity. Common distance metrics used in KNN include Euclidean distance, Manhattan distance, and Minkowski distance. The performance of KNN can vary depending on the characteristics of the data and the problem at hand. It is important to choose a distance metric that is appropriate for the data and aligns with the problem's requirements."
      ],
      "metadata": {
        "id": "6ua_yUR9ja8V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "15. Can KNN handle imbalanced datasets? If yes, how?\n"
      ],
      "metadata": {
        "id": "MKt6vLkXjbBv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "KNN can handle imbalanced datasets by adjusting the class weights or using techniques such as oversampling or undersampling. By assigning different weights to instances based on their class membership, KNN can give more importance to the minority class and prevent it from being overshadowed by the majority class. Resampling techniques can also help balance the classes and improve the performance of KNN on imbalanced datasets."
      ],
      "metadata": {
        "id": "yVqG27zsjbG3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "16. How do you handle categorical features in KNN?"
      ],
      "metadata": {
        "id": "dSoYxch0jbMd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Categorical features in KNN can be handled by either transforming them into numerical representations or using distance metrics specifically designed for categorical data, such as the Hamming distance or the Jaccard distance. One-hot encoding or label encoding can be used to represent categorical features as numerical values that can be used in the distance calculation. It is important to choose an appropriate encoding scheme that preserves the meaningful relationships between different categories."
      ],
      "metadata": {
        "id": "mYcJJvvijbR6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "17. What are some techniques for improving the efficiency of KNN?\n"
      ],
      "metadata": {
        "id": "D9ooBRxljbXT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Techniques for improving the efficiency of KNN include using data structures such as KD-trees or ball trees to speed up the nearest neighbor search. These data structures organize the training data in a way that allows for efficient search and retrieval of nearest neighbors. Another technique is dimensionality reduction, which reduces the number of features while preserving the important information. This can help reduce the computational complexity of the distance calculations."
      ],
      "metadata": {
        "id": "i0d1S5ZBjbc5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "18. Give an example scenario where KNN can be applied."
      ],
      "metadata": {
        "id": "IYKyASjNjbjU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "KNN can be applied in various scenarios, including pattern recognition, image classification, recommendation systems, and anomaly detection. For example, in image classification, KNN can be used to classify new images based on their similarity to a set of labeled images. In recommendation systems, KNN can be used to recommend items to users based on the preferences of their nearest neighbors."
      ],
      "metadata": {
        "id": "_24GlzHNjbpG"
      }
    }
  ]
}